{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Week 4: Word embeddings\n",
    "Identify the word vectors most similar to the vector computed using the analogy:\n",
    "vec(\"woman\") - vec(\"man\") + vec(\"king\")\n",
    "\n",
    "The pretrained word embeddings are loaded from a file. The embeddings are stored in a dictionary where the key is the word and the value is the corresponding vector."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8369c619cd581ef6"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-16T18:58:31.489563800Z",
     "start_time": "2025-04-16T18:58:21.147982900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "file_path = 'glove.6B.100d.txt'\n",
    "\n",
    "embeddings_dict = {}\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        word = parts[0]\n",
    "        vector = np.array(parts[1:], dtype=float)\n",
    "        embeddings_dict[word] = vector\n",
    "\n",
    "print(f\"Loaded {len(embeddings_dict)} word vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define three new vectors for man, woman and king. With these the target vector is calculated."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80740fb4d098822f"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "man_vector = embeddings_dict.get(\"man\")\n",
    "woman_vector = embeddings_dict.get(\"woman\")\n",
    "king_vector = embeddings_dict.get(\"king\")\n",
    "\n",
    "target_vector = woman_vector - man_vector + king_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-16T18:58:31.494560200Z",
     "start_time": "2025-04-16T18:58:31.488562500Z"
    }
   },
   "id": "5713f329d24cdc82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function for finding the (top_n) amount of most similar words to the target vector. The similarity is calculated using cosine similarity."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ddb8aeb9ce31d1c"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to vec('woman') - vec('man') + vec('king'):\n",
      "queen: 0.7834\n",
      "monarch: 0.6934\n",
      "throne: 0.6833\n",
      "daughter: 0.6809\n",
      "prince: 0.6713\n",
      "princess: 0.6644\n",
      "mother: 0.6579\n",
      "elizabeth: 0.6563\n",
      "father: 0.6392\n",
      "wife: 0.6352\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_nearest_words(target_vec, embeddings_dict, top_n=10):\n",
    "    similarities = {}\n",
    "    for word, vec in embeddings_dict.items():\n",
    "        sim = cosine_similarity([target_vec], [vec])[0][0]\n",
    "        similarities[word] = sim\n",
    "\n",
    "    # Sort the words by similarity score in descending order\n",
    "    sorted_words = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_words[1:top_n+1]\n",
    "\n",
    "nearest_words = find_nearest_words(target_vector, embeddings_dict)\n",
    "print(\"Most similar words to vec('woman') - vec('man') + vec('king'):\")\n",
    "for word, score in nearest_words:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-16T19:00:14.735785900Z",
     "start_time": "2025-04-16T18:58:31.495559200Z"
    }
   },
   "id": "1aa9176ea16d5205"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "The five nearest words are queen, monarch, throne, daughter, and prince. This is because the vector operation captures the meaning and relationships between words based on how they are used in text. It changes the meaning of king by removing the male part \"man\" and adding the female part \"woman\", which points the result toward words related to a female royal figure. Words like monarch and throne show up because they are also commonly used when talking about royalty."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "885091f5065fdeaa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
