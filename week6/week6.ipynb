{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Week 6: Sequence-to-sequence model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d32f817861b38aa4"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import ops\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T19:54:20.243393300Z",
     "start_time": "2025-05-07T19:54:20.187402300Z"
    }
   },
   "id": "e87ae3e4c14674f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data\n",
    "The text data read from the textfile and split into pairs of English and Finnish sentences. These text pairs are appended to a list."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dcb698e6a24af3c"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-07T19:54:20.314394500Z",
     "start_time": "2025-05-07T19:54:20.197433200Z"
    }
   },
   "outputs": [],
   "source": [
    "text_file = \"fin.txt\"\n",
    "\n",
    "with open(text_file, encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, finnish, rest = line.split(\"\\t\")\n",
    "    finnish = \"[start] \" + finnish + \" [end]\"\n",
    "    text_pairs.append((english, finnish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"As the crow flies, it's about 20 miles from here.\", '[start] Se on täältä noin 20 mailia linnuntietä. [end]')\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(text_pairs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T19:54:20.315395Z",
     "start_time": "2025-05-07T19:54:20.305395Z"
    }
   },
   "id": "3a54125ff73fa193"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split the data into training, validation, and test sets\n",
    "The text pairs are shuffled randomly and split into training, validation, and test sets. The training set contains 70% of the data, the validation set contains 15%, and the test set contains 15%."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ddd71d0efa1f09e"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T19:54:20.354407500Z",
     "start_time": "2025-05-07T19:54:20.308394800Z"
    }
   },
   "id": "c51137e89c74ad35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Punctuation removed from the text data. The text data is then tokenized using the TextVectorization layer. The TextVectorization layer is used to convert the text data into a sequence of integers, where each integer represents a unique word in the vocabulary. The vocabulary size is set to 15,000, and the maximum sequence length is set to 20."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f1494877363572a"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 30\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_finnish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_finnish_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T19:54:20.593403800Z",
     "start_time": "2025-05-07T19:54:20.358406700Z"
    }
   },
   "id": "70fb3b55dd734958"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, fin):\n",
    "    eng = source_vectorization(eng)\n",
    "    fin = target_vectorization(fin)\n",
    "    return ({\n",
    "                \"english\": eng,\n",
    "                \"finnish\": fin[:, :-1],\n",
    "            }, fin[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, fi_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    fi_texts = list(fi_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, fi_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T19:54:20.855406400Z",
     "start_time": "2025-05-07T19:54:20.604406300Z"
    }
   },
   "id": "242f7fa1ea064435"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 30)\n",
      "inputs['finnish'].shape: (64, 30)\n",
      "targets.shape: (64, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 22:54:13.518475: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['finnish'].shape: {inputs['finnish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T19:54:20.971640Z",
     "start_time": "2025-05-07T19:54:20.855406400Z"
    }
   },
   "id": "460e133d97482b13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define a custom layer for converting the integer sequences into dense vectors. The layer uses an embedding layer to convert the integer sequences into dense vectors. The layer also adds positional embeddings to the dense vectors. The positional embeddings are used to give the model information about the position of each word in the sequence."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53bb05895866e779"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = ops.shape(inputs)[-1]\n",
    "        positions = ops.arange(0, length, 1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return ops.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T19:54:20.978640200Z",
     "start_time": "2025-05-07T19:54:20.970640700Z"
    }
   },
   "id": "fb38da94e15830ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoder and Decoder\n",
    "The Encoder takes the input sequence and produces a sequence of hidden states. The Decoder takes the hidden states from the Encoder and produces the output sequence."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cbdd739b9b7d3f7"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"dense_dim\": self.dense_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T19:54:20.987641Z",
     "start_time": "2025-05-07T19:54:20.975641100Z"
    }
   },
   "id": "f5a06f5d39ebd5e9"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(latent_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n",
    "            padding_mask = ops.minimum(padding_mask, causal_mask)\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = ops.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = ops.arange(sequence_length)[:, None]\n",
    "        j = ops.arange(sequence_length)\n",
    "        mask = ops.cast(i >= j, dtype=\"int32\")\n",
    "        mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = ops.concatenate(\n",
    "            [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],\n",
    "            axis=0,\n",
    "        )\n",
    "        return ops.tile(mask, mult)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"latent_dim\": self.latent_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T19:54:20.987641Z",
     "start_time": "2025-05-07T19:54:20.983640400Z"
    }
   },
   "id": "9ecff705725e05bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the model\n",
    "The model takes two inputs: the Finnish sentence and the English sentence. The model outputs the predicted English sentence."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce61308bcb42f7b5"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "embed_dim = 256 \n",
    "dense_dim = 2048 \n",
    "num_heads = 8 \n",
    "  \n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)     \n",
    " \n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"finnish\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)  \n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)        \n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T19:54:21.427958300Z",
     "start_time": "2025-05-07T19:54:20.989641500Z"
    }
   },
   "id": "386be74a26004eab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we compile and train the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a2a722c5b28d914"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 45ms/step - accuracy: 0.0996 - loss: 5.1001 - val_accuracy: 0.1156 - val_loss: 3.3611\n",
      "Epoch 2/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 26ms/step - accuracy: 0.1143 - loss: 3.5134 - val_accuracy: 0.1293 - val_loss: 2.8059\n",
      "Epoch 3/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 26ms/step - accuracy: 0.1257 - loss: 3.0012 - val_accuracy: 0.1348 - val_loss: 2.5965\n",
      "Epoch 4/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 26ms/step - accuracy: 0.1322 - loss: 2.7073 - val_accuracy: 0.1378 - val_loss: 2.4657\n",
      "Epoch 5/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 26ms/step - accuracy: 0.1372 - loss: 2.5145 - val_accuracy: 0.1371 - val_loss: 2.4929\n",
      "Epoch 6/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 26ms/step - accuracy: 0.1405 - loss: 2.3964 - val_accuracy: 0.1406 - val_loss: 2.4040\n",
      "Epoch 7/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 26ms/step - accuracy: 0.1437 - loss: 2.3050 - val_accuracy: 0.1406 - val_loss: 2.4391\n",
      "Epoch 8/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 27ms/step - accuracy: 0.1456 - loss: 2.2586 - val_accuracy: 0.1406 - val_loss: 2.5342\n",
      "Epoch 9/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 26ms/step - accuracy: 0.1470 - loss: 2.2272 - val_accuracy: 0.1409 - val_loss: 2.5141\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.history.History at 0x7f4ed10eb310>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(\n",
    " optimizer=\"rmsprop\",\n",
    " loss=\"sparse_categorical_crossentropy\",\n",
    " metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "    \n",
    "transformer.fit(train_ds, epochs=30, validation_data=val_ds, verbose=1, callbacks=[early_stopping])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T19:57:54.572502500Z",
     "start_time": "2025-05-07T19:54:21.426958800Z"
    }
   },
   "id": "ee6bab3e729a7a9"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation 1:\n",
      "I got a job.\n",
      "[start] sain töitä [end]\n",
      "\n",
      "Translation 2:\n",
      "Stay absolutely still.\n",
      "[start] pysy ihan vielä ihan vielä [end]\n",
      "\n",
      "Translation 3:\n",
      "Cats usually hate dogs.\n",
      "[start] kissat yleensä [UNK] [end]\n",
      "\n",
      "Translation 4:\n",
      "I felt exhausted when the game was over.\n",
      "[start] minulla oli ihan ollut ihan joka päivä [end]\n",
      "\n",
      "Translation 5:\n",
      "That's a stupid thing to say.\n",
      "[start] se on tyhmä [UNK] [end]\n"
     ]
    }
   ],
   "source": [
    "fin_vocab = target_vectorization.get_vocabulary()\n",
    "fin_index_lookup = dict(zip(range(len(fin_vocab)), fin_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [decoded_sentence]\n",
    "        )[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence]\n",
    "        )\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = fin_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(5):\n",
    "    print(\"Translation \" + str(i + 1) + \":\")\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T19:57:59.283403400Z",
     "start_time": "2025-05-07T19:57:57.776085200Z"
    }
   },
   "id": "253858cc9c8152ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
